{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input  data\n",
    "X = torch.tensor([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])\n",
    "y = torch.tensor([0.0, 1.0, 1.0, 0.0])  # XOR Problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "class LinearModel(nn.Module):\n",
    "  def __init__(self, in_features, out_features):\n",
    "    super(LinearModel, self).__init__()\n",
    "    self.linear = nn.Linear(in_features, out_features)\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.linear(x)\n",
    "\n",
    "# instantiate the model\n",
    "model = LinearModel(2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.2910], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# randomized by default, right?\n",
    "output = model(X[0])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5795], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BinaryClassifer(nn.Module):\n",
    "  def __init__(self, hidden_layer=4):\n",
    "    super(BinaryClassifer, self).__init__()\n",
    "    self.hidden_layer = hidden_layer\n",
    "    self.linear = nn.Linear(2, hidden_layer)\n",
    "    self.hidden = nn.Linear(self.hidden_layer, 1)\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "  def forward(self, x):\n",
    "    output = self.linear(x)\n",
    "    output = torch.relu(output) # ReLU helps a lot.\n",
    "    # Rectifier! Activator!\n",
    "    output = self.hidden(output)\n",
    "    output = self.sigmoid(output)\n",
    "    return output\n",
    "\n",
    "binary_model = BinaryClassifer()\n",
    "bc_output = binary_model(X[0])\n",
    "bc_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my two main questions\n",
    "# how to write the loss function, checking out the `calc_loss_batch` function\n",
    "# I think we can make our not batches a batch by doing `.unsqueeze(0)` or variation\n",
    "# loss = torch.nn.functional.cross_entropy(select_logits, target_batch)\n",
    "# which means we need the selected_logits and the target_batch\n",
    "\n",
    "# iterate the samples\n",
    "# for sample, label in zip(X, y):\n",
    "#   label = label.unsqueeze(0)\n",
    "#   print(sample)\n",
    "#   print(label)\n",
    "#   output = binary_model(sample)\n",
    "#   print(output)\n",
    "#   loss = torch.nn.functional.binary_cross_entropy(output, label)\n",
    "#   # OR BCELoss\n",
    "#   print(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100 loss: 2.7916\n",
      "Epoch 200 loss: 2.7866\n",
      "Epoch 300 loss: 2.7845\n",
      "Epoch 400 loss: 2.7824\n",
      "Epoch 500 loss: 2.7786\n",
      "Epoch 600 loss: 2.7729\n",
      "Epoch 700 loss: 2.7625\n",
      "Epoch 800 loss: 2.7430\n",
      "Epoch 900 loss: 2.7057\n",
      "Epoch 1000 loss: 2.6530\n"
     ]
    }
   ],
   "source": [
    "# how to write the training loop\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = optim.SGD(binary_model.parameters(), lr=learning_rate)\n",
    "epochs = 1000\n",
    "# can we just drop an Adam in here?\n",
    "# optimizer = optim.Adam(binary_model.parameters(), lr=learning_rate)\n",
    "# epochs = 1000\n",
    "# optimizer = optim.AdamW(binary_model.parameters(), lr=learning_rate)\n",
    "# epochs = 1000\n",
    "\n",
    "# loss_function = nn.BCELoss()\n",
    "\n",
    "# Training Loop!\n",
    "for epoch in range(epochs):\n",
    "  epoch_loss = 0.0\n",
    "  for sample, label in zip(X, y):\n",
    "    sample = sample #.unsqueeze(0)\n",
    "    label = label.unsqueeze(0)\n",
    "\n",
    "    # Forward pass\n",
    "    output = binary_model(sample)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = torch.nn.functional.binary_cross_entropy(output, label)\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad() # 0 the grads out\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Accumlate loss\n",
    "    epoch_loss += loss.item()\n",
    "\n",
    "  if (epoch + 1) % 100 == 0:\n",
    "  # if (epoch + 1) % 10 == 0:\n",
    "    print(f'Epoch {epoch + 1} loss: {epoch_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction.item()=0.0\n",
      "prediction.item()=1.0\n",
      "prediction.item()=1.0\n",
      "prediction.item()=0.0\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "with torch.no_grad():\n",
    "  for sample, label in zip(X, y):\n",
    "    output = binary_model(sample)\n",
    "    prediction = (output > 0.5).float()\n",
    "    print(f\"{prediction.item()=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cool, that's pretty neat\n",
    "# whoa, changed optimizer to Adam and got convergence much faster\n",
    "# ah, I needed to re-create the model\n",
    "# it was using SGD's work.\n",
    "# okay after a reset it was at least still 5x faster.\n",
    "# result: 0.0434 loss after 1000 epochs with AdamW\n",
    "# result: 0.0909 loss after 1000 epochs with Adam\n",
    "# result: 2.6530 loss after 1000 epochs with SGD\n",
    "\n",
    "# I've read Adam models take up more memory, because they hold two new parameters for each parameter\n",
    "# that already exists, but boy they are fast and efficient on this super small dataset.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
